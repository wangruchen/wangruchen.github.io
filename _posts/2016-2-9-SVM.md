---
layout: default
title: Support Vector Machine学习整理
---

# {{ page.title }}


<script type="text/javascript"
 src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

**支持向量机（Support Vector Machine, SVM）**：在特征空间中构建一个最大间隔超平面（<img src="http://www.forkosh.com/mathtex.cgi? wx+b=0">，x为训练样本）作为分类的边界，将样本点分为两类（<img src="http://www.forkosh.com/mathtex.cgi? y=\sign(wx+b)">，y为类别）。

<!--## 函数间隔（functional margin）和几何间隔（geometrical margin）-->

## 支持向量机中的一些概念

- **间隔**：特征空间中样本点到超平面的距离。

- **函数间隔（Functional margin）**：样本点与超平面间的函数间隔为
$$\hat{\gamma}=y(\omega^{T}x+b)=yf(x)$$，它可以表示点到超平面距离的远近。超平面关于训练集的函数间隔为所有样本点到超平面中间隔最小值。然而，当w和b等比例变化时，超平面没有变，但函数间隔会变化。因此引入了几何间隔。

- **几何间隔（Geometrical margin）**：为函数间隔除以<img src="http://www.forkosh.com/mathtex.cgi? ||\omega||">,
$$
\tilde{\gamma}=\frac{\hat{\gamma}}{||\omega||}
$$

- **最大间隔超平面**：使得距离超平面最近的样本点与该超平面的几何间隔最大时确定的超平面为最大间隔超平面。寻找最大间隔超平面的目标函数为
$$
\max\tilde{\gamma}
$$

- **支持向量**：距离最大间隔超平面最近的样本点为支持向量。在决定分离超平面时，只有支持向量起作用。

- **核函数**：在支持向量机中，对于线性不可分的情况，通过核函数将训练样本映射到一个高维空间中，在这个空间中构造最优分类超平面。

- **结构风险最小**：结构化风险=经验风险+置信风险。经验风险为分类器在训练集上的误差。置信风险=分类器在测试集上的误差。影响置信风险的因素：样本数量，训练集样本越多，学习结果越有可能正确，置信风险小；分类函数的VC维，VC维越大，推广能力越差，置信风险越大。

- **VC维**：可以被散列的点的最大数量。反映了函数集的学习能力，VC维越大则学习机器学习越复杂。

- **拉格朗日对偶性**：拉格朗日对偶性是解带约束的最优化问题的方法，在应用中通过拉格朗日对偶原理将原始问题转换为对偶问题。

	原始问题：假设原始问题如下：
	 
	 $$
	 \min_{x} f(x), s.t. g(x)\le0
	 $$
	 引进广义拉格朗日函数：
	 $$
	 L(x,\lambda)=f(x)+\lambda g(x), \lambda\ge0
	 $$
	 那么原问题等价于：
	 $$
	 \min_{x}\max_{\lambda:\lambda\ge0} L(x,\lambda)
	 $$
	 
	 对偶问题：将原问题极小极大顺序互换后的极大极小问题称为原始问题的对偶问题：
	 $$
	 \max_{\lambda:\lambda\ge0}\min_{x}L(x,\lambda)
	 $$
	 

- **KKT条件（Karush-Kuhn-Tucker）**：对含有不等式约束的优化问题，如何求最优值？常用的方法是KKT条件。

## 支持向量机算法

SVM通过找到一个最优分类



### 线性可分

SVM通过找到一个最大间隔超平面将样本分为两类。超平面可以表示为<img src="http://www.forkosh.com/mathtex.cgi? f(x)=w^{T}x+b">，<img src="http://www.forkosh.com/mathtex.cgi? f(x)">大于0的点对应类别为1，<img src="http://www.forkosh.com/mathtex.cgi? f(x)">小于0的点对应的类别为-1。
   
怎样确定最大间隔超平面？找到权值向量w和阈值b的最优值，使到超平面最近的样本点距离超平面最远，即几何间隔最大：
$$
\max\tilde{\gamma}=\max\frac{\hat{\gamma}}{||\omega||}=\max\frac{y(\omega^{T}x+b)}{||\omega||}
$$

令<img src="http://www.forkosh.com/mathtex.cgi? \hat{\gamma}=1">，则目标函数转化为：
$$\max\frac{1}{||\omega||}, s.t., y_{i}(\omega^{T}x_{i}+b)\ge1, i=1,\dots,n$$

这个目标函数等价于：
$$
\min\frac{1}{2}||\omega||^{2}, s.t., y_{i}(\omega^{T}x_{i}+b)\ge1, i=1,\dots,n
$$

这个凸优化问题通过拉格朗日对偶变换到对偶变量（这种方法效率高）：
$$
L(\omega,b,\alpha)=\frac{1}{2}||\omega||^{2}-\sum^{n}_{i=1}\alpha_{i}[y_{i}(\omega^{T}x_{i}+b)-1]
$$ 

令<img src="http://www.forkosh.com/mathtex.cgi? \theta(\omega)=\max_{\alpha_{i}\ge0}L(\omega,b,\alpha)">，当所有约束条件都满足时，则有<img src="http://www.forkosh.com/mathtex.cgi? \theta(\omega)=\frac{1}{2}||\omega||^{2}">。因此目标函数变为：
$$
\min_{\omega,b}\theta(\omega)=\min_{\omega,b}\max_{\alpha_{i}\ge0}L(\omega,b,\alpha)
$$
将最小和最大位置交换一下得到：
$$
\max_{\omega,b}\min_{\alpha_{i}\ge0}L(\omega,b,\alpha)
$$
在满足KKT条件下，解上式相当于间接的求解目标函数。
求解上式，首先让L关于w和b最小化：
$$
\frac{\partial L}{\partial\omega}=0\Rightarrow \omega=\sum^{n}_{i=1}\alpha_{i}y_{i}x_{i}
$$
$$
\frac{\partial L}{\partial b}=0\Rightarrow \sum^{n}_{i=1}\alpha_{i}y_{i}=0
$$
带回到L得到：
$$
L(\omega,b,\alpha)=\sum^{n}_{i=1}\alpha_{i}-\frac{1}{2}\sum^{n}_{i,j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}x^{T}_{i}x_{j}=\sum^{n}_{i=1}\alpha_{i}-\frac{1}{2}\sum^{n}_{i,j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i},x_{j})
$$

因此得到：
$$
\max L(\omega,b,\alpha)=\max\sum^{n}_{i=1}\alpha_{i}-\frac{1}{2}\sum^{n}_{i,j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i},x_{j})
$$
$$
s.t. \sum^{n}_{i=1}y_{i}\alpha_{i}=0, 0\le \alpha_{i} \le C
$$

以上为不等式约束的二次函数极值问题（Quadratic Programming, QP）。由Kuhn Tucker定理可知，上面的最优解必须满足KKT条件。

利用SMO算法可以求解<img src="http://www.forkosh.com/mathtex.cgi? \alpha_{i}">，求得了它就可以得到w和b（b根据最优权值向量和一个正的支持向量可以求出，<img src="http://www.forkosh.com/mathtex.cgi? b=1-W_{0}^{T}X^{s}">）。

代入到分类函数：
$$
f(x)=(\sum^{n}_{i=1}\alpha_{i}y_{i}x_{i})^{T}x+b=\sum^{n}_{i=1}\alpha_{i}y_{i}\<x_{i},x\>+b
$$
中可以得到预测的类别结果。

<!--在求<img src="http://www.forkosh.com/mathtex.cgi? L(\omega,b,\alpha)">关于w和b最小，即关于<img src="http://www.forkosh.com/mathtex.cgi? \alpha">极大，可以利用SMO算法求解拉格朗日乘子<img src="http://www.forkosh.com/mathtex.cgi? \alpha">-->

<!--将w带入到<img src="http://www.forkosh.com/mathtex.cgi? f(x)=w^{T}x+b">中得到：
$$
f(x)=\sum^{n}_{i=1}\alpha_{i}y_{i}\<x_{i},x\>+b=\sum^{n}_{i=1}\alpha_{i}y_{i}K(x_{i},x_{j})+b
$$-->

### 非线性情况

对于非线性情况，SVM的处理方法是选择一个核函数，通过将数据映射到高维空间中，来解决在原始空间中线性不可分的问题。

若到高维空间的映射为<img src="http://www.forkosh.com/mathtex.cgi? \phi">，则分类函数为：

$$
f(x)=\sum^{n}_{i=1}\omega_{i}\phi_{i}(x)+b=\sum^{n}_{i=1}\alpha_{i}y_{i}\<\phi(x_{i})·\phi(x)\>+b
$$

核函数为计算两个向量在隐式映射过后的空间中的内积函数：

$$
K(x,z)=\<\phi(x)·\phi(z)\>
$$

其中<img src="http://www.forkosh.com/mathtex.cgi? \alpha">由如下计算得到：

$$
\max\sum^{n}_{i=1}\alpha_{i}-\frac{1}{2}\sum^{n}_{i,j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}k(x_{i},x_{j})
$$
$$
s.t., \alpha_{i}\ge0, i=1,\dots,n
$$
$$
\sum^{n}_{i=1}\alpha_{i}y_{i}=0
$$



## Kernel Function (核函数)

### 常用的核函数


#### Linear Kernel (线性核)
主要用于线性可分的情形。参数较少（没有专门需要设置的参数），速度快。

$$
k(x_{1},x_{2})=\<x_{1},x_{2}\>
$$
实际上就是原始空间的内积。（其实就是线性情况，不用核函数）

#### Polynomial Kernel (多项式核)
三个参数：degree用来设置多项式核函数的最高次项次数，LIBSVM中默认值是3；gamma用来设置公式中的第一个r（gamma），LIBSVM中默认值是<img src="http://www.forkosh.com/mathtex.cgi? \frac{1}{k}">(k是类别数)；coef0用来设置公式中的第二个R，LIBSVM中默认值是0。

$$
k(x_{1},x_{2})=(r\<x_{1},x_{2}\>+R)^{d}
$$


#### Gaussian Kernel (高斯核)
主要用于线性不可分的情形。参数多，分类结果非常依赖于参数（通常通过交叉验证来寻找适合的参数）。gamma用来设置公式中的第一个r（gamma），LIBSVM中默认值是<img src="http://www.forkosh.com/mathtex.cgi? \frac{1}{k}">(k是类别数)；惩罚因子C。

$$
k(x_{1},x_{2})=e^{(-r\frac{||x_{1}-x_{2}||^{2}}{2\sigma^{2}})}
$$

#### Sigmoid Kernel
两个参数：gamma用来设置公式中的第一个r（gamma），LIBSVM中默认值是<img src="http://www.forkosh.com/mathtex.cgi? \frac{1}{k}">(k是类别数)；coef0用来设置核函数中的第二个R，LIBSVM中默认值是0。

$$
k(x_{1},x_{2})=\tanh(r\<x_{1},x_{2}\>+R)
$$

其他核函数[http://blog.csdn.net/chlele0105/article/details/17068949](http://blog.csdn.net/chlele0105/article/details/17068949)

针对核函数参数的选择可以采用交叉验证或网格搜索法进行选择

### 核函数使用步骤

1. 选择核函数。
2. 用核函数将样本变换为核函数矩阵，即将样本映射到高维特征空间。
3. 在高维特征空间中找到线性分类超平面。

### 核函数的选择
常用的核函数选择方法有：

1. 根据经验先验知识预先选定核函数
2. 采用交叉验证的方法，即在进行核函数选取时，分别试用不同的核函数，归纳误差最小的核函数就是最好的核函数；在选择参数时使用网格搜索法，然后采用交叉验证找到最优的参数。（网格搜索法就是采用n层循环，n是参数的个数）

## 松弛变量
有时数据中会存在偏离正常位置很远的数据点，成为outlier。它们的存在可能会对超平面的确定有很大影响。为了处理这种情况，SVM允许数据点在一定程度上偏离超平面。因此约束条件变为：
$$
y_{i}(w^{T}x_{i}+b)\ge1-\xi_{i}
$$
其中<img src="http://www.forkosh.com/mathtex.cgi? \xi_{i}">为松弛变量。

引入松弛变量后，由于<img src="http://www.forkosh.com/mathtex.cgi? \xi_{i}">不可以任意大，因此目标函数为：
$$
\min\frac{1}{2}||\omega||^{2}+C\sum^{n}_{i=1}\xi_{i}
$$
$$
s.t., y_{i}(w^{T}x_{i}+b)\ge1-\xi_{i}
$$
$$
\xi_{i}\ge0, i=1,\dots,n
$$

得到的拉格朗日函数为：
$$
L(\omega,b,\xi,\alpha,r)=\frac{1}{2}||\omega||^{2}+C\sum^{n}_{i=1}\xi_{i}-\sum^{n}_{i=1}\alpha_{i}(y_{i}(\omega^{T}x_{i}+b)-1+\xi_{i})-\sum^{n}_{i=1}r_{i}\xi_{i}
$$
求L对w、b和<img src="http://www.forkosh.com/mathtex.cgi? \xi_{i}">最小化。

$$
\frac{\partial L}{\partial\omega}=0\Rightarrow \omega=\sum^{n}_{i=1}\alpha_{i}y_{i}x_{i}
$$
$$
\frac{\partial L}{\partial b}=0\Rightarrow \sum^{n}_{i=1}\alpha_{i}y_{i}=0
$$
$$
\frac{\partial L}{\partial\xi_{i}}=0\Rightarrow C-\alpha_{i}-r_{i}=0
$$
将它们带入到L得到目标函数：

$$
\max\sum^{n}_{i=1}\alpha_{i}-\frac{1}{2}\sum^{n}_{i,j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i},x_{j})
$$
$$
s.t., 0\le\alpha_{i}\le C, i=1,\dots,n
$$
$$
\sum^{n}_{i=1}\alpha_{i}y_{i}=0
$$







## Sequential minimal optimization (SMO，序列最小优化算法)

SMO是一种解决此类支持向量机优化问题的迭代算法。由于目标函数为凸函数，一般的优化算法都通过梯度方法一次优化一个变量求解二次规划问题的最大值。

前面最后留下来的一个对偶函数优化问题为：
$$
\max L(\omega,b,\alpha)=\sum^{n}_{i=1}\alpha_{i}-\frac{1}{2}\sum^{n}_{i,j=1}\alpha_{i}\alpha_{j}y_{i}y_{j}K(x_{i},x_{j})
$$
$$
s.t. \sum^{n}_{i=1}y_{i}\alpha_{i}=0, 0\le \alpha_{i} \le C
$$
SMO就是要解这个凸二次规划问题


参考文章：[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/v_july_v/article/details/7624837)


<!--线性分类：

1. 已知类别的训练集样本，来确定最优分类超平面<img src="http://www.forkosh.com/mathtex.cgi? wx+b=0">，因此要求w,b。
2. 要求最优分类超平面的w,b，即使几何间隔最大。在使几何间隔最大求解的w,b的过程中，用到拉格朗日函数、原始对偶问题、凸二次规划等问题。

非线性分类：对于线性不可分的情况，选定核函数将样本映射到高维空间中，构建超平面进行分割。


序列最小优化算法（Sequential Minimal Optimization, SMO）是一种用于解决支持向量机训练过程中产生优化问题的算法。-->
